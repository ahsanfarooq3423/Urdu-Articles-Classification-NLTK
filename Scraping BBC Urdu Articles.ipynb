{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(r'/Users/ahsan/Documents/git/Urdu-Article-Classification')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The two links i.e for sports and science from which articles url are scraped\n",
    "sport = 'https://www.bbc.com/urdu/search/?q=%DA%A9%DA%BE%DB%8C%D9%84&start='\n",
    "science = 'https://www.bbc.com/urdu/search/?q=%D8%B3%D8%A7%D8%A6%D9%86%D8%B3&start='\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes the following parameters:\n",
    " # url, the pages from which the actual articles are scraped\n",
    " # n : n is just the number, the greater the number the more article links it will fetch\n",
    " # category : for now their are only two categories i.e sports and science\n",
    "def get_urls(url,n,category):\n",
    "    urls = []\n",
    "    for i in range(n):\n",
    "        temp = url + str(i) + '1'\n",
    "        html_page = request.urlopen(temp).read().decode('utf8')\n",
    "        soup = BeautifulSoup(html_page)\n",
    "        link = \"^https://www.bbc.co.uk/urdu/\" + category\n",
    "        for link in soup.findAll('a', attrs={'href': re.compile(link)}):\n",
    "            urls.append(link.get('href'))\n",
    "    urls = list(set(urls))\n",
    "    return urls\n",
    "        \n",
    "    \n",
    "    \n",
    "#This function takes the array of the articles link and then scrape the article content \n",
    "#This function then return the two-d array in which the sub-array contains each article\n",
    "def get_urdu_article(urls):\n",
    "    articles = []\n",
    "    for url in urls:\n",
    "        html = request.urlopen(url).read().decode('utf8')\n",
    "        raw = BeautifulSoup(html, 'html.parser').get_text()\n",
    "        raw = raw.translate ({ord(c): \" \" for c in \"\\\\!@#$%^&*\\'\\\"()[]{};:,./<>?\\|`~-=_+\\n\"})\n",
    "        tokens = word_tokenize(raw)\n",
    "        tokens = [w for w in tokens if not re.match(r'[A-Z]+', w, re.I)]\n",
    "        tokens = [w for w in tokens if not re.match(r'[0-9]+', w, re.I)]\n",
    "        articles.append(tokens)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Getting the URLs for Science Articles.</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "science_urls = get_urls(science,4,'science')\n",
    "len(science_urls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Getting the URLs for Sports Articles.</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sports_urls = get_urls(sport,4,'sport')\n",
    "len(sports_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Getting the Content of the science articles.</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "science_articles = get_urdu_article(science_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Getting the Content of the Sports articles.</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sports_articles = get_urdu_article(sports_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Writing the scraped data in csv files</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_articles = np.array(sports_articles)\n",
    "pd.DataFrame(sp_articles).to_csv(\"sports.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_articles = np.array(science_articles)\n",
    "pd.DataFrame(sc_articles).to_csv(\"science.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
